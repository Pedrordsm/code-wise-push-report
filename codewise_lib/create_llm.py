import os
import sys
from dotenv import load_dotenv

#import dos modelos de api que o codewise vai suportar
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_openai import ChatOpenAI

# trata erros de importa√ß√£o caso as libs n√£o estejam instaladas
# instalar: pip install -r requirements.txt (adicionar as outras llms no requirements.txt)
except ImportError:
    print("Erro: Bibliotecas de LLM (ex: langchain-google-genai, langchain-openai) n√£o encontradas.", file=sys.stderr)
    print("Instale as depend√™ncias necess√°rias (verifique o requirements.txt).", file=sys.stderr)
    sys.exit(1)

def create_llm():
    """
    L√™ as vari√°veis de ambiente e instancia o modelo de LLM selecionado.
    """
    load_dotenv()

    provider = os.getenv("AI_PROVIDER", "google").lower() # s√≥ de garantia, se n√£o for definido, setei a do goodle como padr√£o
     # o modelo acho que √© v√°lido ser opcional, pro usu√°rio escolher o modelo padr√£o do provider se quiser 
     # acho que resolve aquela quest√£o de querer o gemini pro, fica a crit√©rio do usu√°rio
    model_name = os.getenv("AI_MODEL") 

    print(f"--- ü§ñ Inicializando IA com o provedor: {provider} ---", file=sys.stderr)

    try:
        if provider == "google":
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                print("Erro: AI_PROVIDER='google', mas GEMINI_API_KEY n√£o foi definida no .env", file=sys.stderr)
                sys.exit(1)
            
            model = model_name or "gemini-2.0-flash" # se n√£o for definido, usa a vers√£o gratuita como padr√£o
            print(f"Usando Google (Gemini) - Modelo: {model}", file=sys.stderr)
            return ChatGoogleGenerativeAI(
                model_name=model,
                google_api_key=api_key
            )

        elif provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("Erro: AI_PROVIDER='openai', mas OPENAI_API_KEY n√£o foi definida no .env", file=sys.stderr)
                sys.exit(1)
            
            model = model_name or "gpt-4o-mini" #mesma coisa, usa esse modelo como padr√£o se n√£o for definido um espec√≠ficosw
            print(f"Usando OpenAI - Modelo: {model}", file=sys.stderr)
            return ChatOpenAI(
                model_name=model,
                api_key=api_key
            )
        
        # se quisermos, da pra adicionar outras llms aqui

        else:
            print(f"Erro: AI_PROVIDER '{provider}' n√£o √© suportado. (Use 'google' ou 'openai')", file=sys.stderr)
            sys.exit(1)

    except Exception as e:
        print(f"Erro ao inicializar o LLM para o provider '{provider}'.", file=sys.stderr)
        print(f"Verifique suas chaves de API e se o modelo '{model_name}' √© v√°lido.", file=sys.stderr)
        print(f"Erro original: {e}")
        sys.exit(1)